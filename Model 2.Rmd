---
title: "Model 2"
author: "Idzhar A. Lakibul"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Helper packages

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(keras)
library(caret)
library(rsample)   
library(recipes)   
library(h2o)
library(readr)
library(ranger)
library(gbm)
library(tfruns)
library(tfestimators)
```


```{r, message=FALSE}
radiomics_completedata <- read_csv("C:/STAT 371/Final Project/radiomics_completedata.csv")
 View(radiomics_completedata)
 Rad<-radiomics_completedata
```

```{r, message=FALSE}
Rad <- Rad[,-1]
Rad
```

```{r, message=FALSE}
Rad <- na.omit(Rad)
```


```{r, message=TRUE}

index<-createDataPartition(Rad$Failure.binary,p=0.7,list=F)


y_train <- Rad[index,1]
y_test <- Rad[-index,1]


y_train<-to_categorical(y_train, num_classes = 10)
y_test<-to_categorical(y_test, num_classes = 10)


```


```{r, message=TRUE}
x_train <- data.matrix(Rad[index,-1])
x_test <- data.matrix(Rad[-index,-1])
as.matrix(apply(x_train, 2, function(x) (x-min(x))/(max(x) - min(x)))) -> x_train
as.matrix(apply(x_test, 2, function(x) (x-min(x))/(max(x) - min(x)))) -> x_test

```

Build the model
Create three hidden layers with 256, 128, 128, 64 and 64 neurons, 
respectively with activation functions of Sigmoid
Every layer is followed by a dropout to avoid overfitting 

```{r, message=TRUE}
model <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "sigmoid", input_shape = c(784)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "sigmoid") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "sigmoid") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "sigmoid") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "sigmoid") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = "softmax") %>% 
  

```

Backpropagation compiler approach 
```{r, message=TRUE}
compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
  )

```

Train the model with epoch = 10, batch size = 128 and validation split = 0.15
```{r, message=TRUE}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

history <- model %>% 
  fit(x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.15)

```

Evaluate the trained model using the testing dataset.
```{r, message=TRUE}
model %>%
  evaluate(x_test, y_test)
```

Get the model prediction using the testing dataset.
```{r, message=TRUE}
model %>%
  predict_classes(x_test)
```